from datetime import datetime

def format_russian_datetime(dt_str):
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º +0300 –Ω–∞ +03:00
    if len(dt_str) > 5 and (dt_str[-5] == '+' or dt_str[-5] == '-') and dt_str[-2] != ':':
        dt_str = dt_str[:-2] + ':' + dt_str[-2:]
    dt = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
    months = [
        '', '—è–Ω–≤–∞—Ä—è', '—Ñ–µ–≤—Ä–∞–ª—è', '–º–∞—Ä—Ç–∞', '–∞–ø—Ä–µ–ª—è', '–º–∞—è', '–∏—é–Ω—è',
        '–∏—é–ª—è', '–∞–≤–≥—É—Å—Ç–∞', '—Å–µ–Ω—Ç—è–±—Ä—è', '–æ–∫—Ç—è–±—Ä—è', '–Ω–æ—è–±—Ä—è', '–¥–µ–∫–∞–±—Ä—è'
    ]
    return f"{dt.day} {months[dt.month]} –≤ {dt.strftime('%H:%M')}"
import requests
from bs4 import BeautifulSoup
import time
import os

FORUM_URLS = {
    "–í–µ—Ä—Ö–æ–≤–Ω—ã–π —Å—É–¥": "https://forum.gta5rp.com/forums/verxovnyi-sud.1834/",
    "–û–∫—Ä—É–∂–Ω–æ–π —Å—É–¥": "https://forum.gta5rp.com/forums/okruzhnoi-sud.1835/",
    "–£–î–û –∏ –†–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è –æ—Å—É–∂–¥—ë–Ω–Ω—ã—Ö": "https://forum.gta5rp.com/forums/udo-i-reabilitacija-osuzhdennyx.1838/",
}
WEBHOOK_URL = "https://discord.com/api/webhooks/1424215766445719652/XJmp_Y7f6KTNJ41eKxaQAD0z0KJax4jhl2-SdIrtssoBqRdL6Ty4olwF_VAZGXhR0Ten"
TOPIC_SELECTOR = "div.structItem-title > a:not(.labelLink)"
NAME_SELECTOR = "ul.structItem-parts > li:not(.structItem-startDate) > a"
TIME_SELECTOR = "li.structItem-startDate > a > time"
CHECK_INTERVAL = 60
SEEN_FILE = "supremecourt_links.txt"
webhook_url = "https://discord.com/api/webhooks/1424215766445719652/XJmp_Y7f6KTNJ41eKxaQAD0z0KJax4jhl2-SdIrtssoBqRdL6Ty4olwF_VAZGXhR0Ten"

def get_all_forum_urls():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö URL —Ñ–æ—Ä—É–º–æ–≤"""
    return list(FORUM_URLS.values())

def get_all_links():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (—Å—Å—ã–ª–∫–∞, –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–µ–º—ã, —Ñ–æ—Ä—É–º, –∞–≤—Ç–æ—Ä, –≤—Ä–µ–º—è)"""
    results = []
    for forum_title, url in FORUM_URLS.items():
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        topics = soup.select(TOPIC_SELECTOR)
        authors = soup.select(NAME_SELECTOR)
        times = soup.select(TIME_SELECTOR)
        for topic, author, timestamp in zip(topics, authors, times):
            href = topic.get("href")
            if href:
                results.append((href, topic.text.strip(), forum_title, author.text.strip(), timestamp.get("datetime", str(timestamp))))
    return results

def send_to_discord(forum_title, tag_list, topic_title, author, timestamp, link):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ Discord —á–µ—Ä–µ–∑ –≤–µ–±—Ö—É–∫"""
    formatted_time = format_russian_datetime(timestamp)
    data = {
        
        "content": ", ".join(tag_list),
        "embeds": [
            {
                "title": f"{forum_title}: {topic_title}",
                "description": link,
                "color": int("2f3136", 16),
                "fields": [
                    {
                        "name": "–ü–æ–¥–∞–Ω–æ",
                        "value": formatted_time,
                        "inline": False
                    },
                    {
                        "name": "–ê–≤—Ç–æ—Ä",
                        "value": str(author),
                        "inline": False
                    }
                ],
                "footer": {
                    "text": "üßø Created by Eugen Norman | Discord for communication - eugen_op."
                }
            }
        ]
    }
    response = requests.post(WEBHOOK_URL, json=data)
    time.sleep(1)
    response.raise_for_status()

# --- –ü–ï–†–í–´–ô –ó–ê–ü–£–°–ö ---
if not os.path.exists(SEEN_FILE):
    print("‚è≥ –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫: —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—É—â–∏–µ —Ç–µ–º—ã (–Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º)")
    links = get_all_links()
    seen_links = set()
    for href, _ in links:
        for forum_url in get_all_forum_urls():
            if not href.startswith("http"):
                href = requests.compat.urljoin(forum_url, href)
        seen_links.add(href)
    with open(SEEN_FILE, "w") as f:
        for href in seen_links:
            f.write(href + "\n")
    print("‚úÖ –°—Ç–∞—Ä—ã–µ —Ç–µ–º—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã, —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ.")
else:
    with open(SEEN_FILE, "r") as f:
        seen_links = set(line.strip() for line in f)

print("‚úÖ –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω. –û—Ç—Å–ª–µ–∂–∏–≤–∞—é —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–µ–º—ã...")

# --- –û–°–ù–û–í–ù–û–ô –¶–ò–ö–õ ---
while True:
    try:
        links = get_all_links()
        for href, title, forum_title, author, timestamp in links:
            match forum_title:
                case "–í–µ—Ä—Ö–æ–≤–Ω—ã–π —Å—É–¥":
                    tag_list = ["<@&1332717305154637896> <@&1316096152113905665>"]
                case "–û–∫—Ä—É–∂–Ω–æ–π —Å—É–¥":
                    tag_list = ["<@&1316096160443666504>"]
                case "–£–î–û –∏ –†–µ–∞–±–∏–ª–∏—Ç–∞—Ü–∏—è –æ—Å—É–∂–¥—ë–Ω–Ω—ã—Ö":
                    tag_list = ["<@&1316096152113905665> <@&1316096203385213001>"]
            for forum_url in get_all_forum_urls():
                if not href.startswith("http"):
                    href = requests.compat.urljoin(forum_url, href)
            if href not in seen_links:
                # –ù–æ–≤–∞—è —Ç–µ–º–∞ ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ Discord
                send_to_discord(forum_title, tag_list, title, author, timestamp, href)
                print(f"üì® {forum_title}: {title}")

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏ —Ñ–∞–π–ª
                seen_links.add(href)
                with open(SEEN_FILE, "a") as f:
                    f.write(href + "\n")

        time.sleep(CHECK_INTERVAL)

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: {e}")
        time.sleep(CHECK_INTERVAL)